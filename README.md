# Architecture Design

This document compares the architecture design of the following LLM-D configuration

**gpt-oss-20b (single pool)**

https://github.com/redhat-na-ssa/demo-ocp-llm-d/blob/main/gitops/instance/llm-d/gpt-oss-20b/llm-infra.yaml

**qwen-pd (prefill/decode split)**

https://github.com/redhat-na-ssa/demo-ocp-llm-d/blob/main/gitops/instance/llm-d/pd-disaggregation/pd-deployment.yaml

### gpt-oss-20b (single pool)

Client â†’ Router â†’ [Single LLM Pod] â†’ Response

### qwen-pd (prefill/decode split)

Client â†’ Router â†’ Prefill Pool (prompt encoding)
â†“ (KV cache via RDMA)
Decode Pool (token generation)
â†“
Response


| Aspect                | `gpt-oss-20b`YAML                                                 | `qwen-pd`YAML                                                                |
| --------------------- | ----------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| **Architecture Type** | **Single pool**(monolithic inference)                             | Prefill/Decode (PD) separated(two-stage pipeline)                            |
| **Purpose**           | Simpler LLM serving â€” one pod does all inference work.           | Optimized for**parallelism and KV cache reuse**across two specialized pools. |
| **Result**            | Easier to deploy but slower for long context or high concurrency. | More complex, but scales better for multi-request or large-token workloads.  |

**Explanation:**

* The **single-pool setup** (gpt-oss-20b) means every request is processed entirely by one model server (prefill + decode).
* The **PD-separated setup** (qwen-pd) splits these stages:
  * *Prefill pool*: Handles initial prompt embeddings (context).
  * *Decode pool*: Handles token-by-token generation.

This PD split allows **KV cache transfer** between stages, improving throughput on large models.

# Router Scheduler Config (`EndpointPickerConfig`)


| Component               | `gpt-oss-20b`                                                                                      | `qwen-pd`                                                                                      |
| ----------------------- | -------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| **Plugin Types**        | `single-profile-handler`,`max-score-picker`,`queue-scorer`,`kv-cache-scorer`,`prefix-cache-scorer` | `pd-profile-handler`,`prefill-header-handler`,`prefill-filter`,`decode-filter`,`random-picker` |
| **Purpose**             | Focus on**cache-aware routing**(prefix/kv scoring for token reuse).                                | Focus on**stage separation and routing**Â between prefillÂ andÂ decodeÂ pools.                 |
| **Scheduling Profiles** | Single`"default"`profile (all handled together).                                                   | Two profiles:`"prefill"`and`"decode"`.                                                         |

**Explanation:**

* `gpt-oss-20b` router tries to route requests to servers with **matching prefixes or cached KV states** â€” optimizing for *latency and cache reuse* within a single model.
* `qwen-pd` uses a **PD pipeline**, where the router first decides if a request is prefill or decode and routes accordingly.

# Model Deployment Resource


| Aspect             | `gpt-oss-20b`            | `qwen-pd`                                   |
| ------------------ | ------------------------ | ------------------------------------------- |
| **CPU/Memory**     | 1 CPU / 8 Gi             | 4 CPU / 32 Gi                               |
| **GPU**            | 1 GPU                    | 1 GPU (per pool)                            |
| **Extra Resource** | None                     | `rdma/roce_gdr`(for**RDMA over RoCE**)      |
| **Volume**         | Uses PVC (`gpt-oss-20b`) | None                                        |
| **Networking**     | Normal TCP/IP            | **RDMA network**for low-latency KV transfer |

**Explanation:**

* `qwen-pd` leverages **RDMA (Remote Direct Memory Access)** to move KV cache data between prefill and decode pods *without CPU involvement* â€” drastically improving speed for distributed inference.
* `gpt-oss-20b` runs entirely in one pod â€” simpler, but lacks distributed acceleration.

# Environment Variable


| Variable               | `gpt-oss-20b`                     | `qwen-pd`                                            |
| ---------------------- | --------------------------------- | ---------------------------------------------------- |
| `VLLM_ADDITIONAL_ARGS` | Only disables uvicorn access log. | Enables RDMA KV cache transfer using`NixlConnector`. |
| `KSERVE_INFER_ROCE`    | â€”                                | `"true"`(activates RDMA mode).                       |
| `UCX_*`                | â€”                                | UCX transport settings for RDMA communication.       |

**Explanation:**
The `qwen-pd` setup uses **UCX** (Unified Communication X) â€” a high-performance library used in HPC and distributed AI inference â€” to facilitate fast data transfer.

# LLM Serving Strategy Summary


| Strategy                      | Description                                          | Pros                                                                            | Cons                                                |
| ----------------------------- | ---------------------------------------------------- | ------------------------------------------------------------------------------- | --------------------------------------------------- |
| **Single-pool (gpt-oss-20b)** | Each pod handles full requests.                      | Simpler, fewer moving parts.                                                    | Lower throughput for long or parallel prompts.      |
| **PD-split (qwen-pd)**        | Dedicated prefill and decode pods exchange KV cache. | Higher concurrency, faster generation for long sequences.When to Use Each<br /> | Requires RDMA networking and complex orchestration. |

# When to Use Each


| Use Case                                                              | Choose        |
| --------------------------------------------------------------------- | ------------- |
| Simple LLM inference (dev/test, small models)                         | gpt-oss-20b`  |
| High-performance inference (long prompts, high QPS, distributed GPUs) | qwen-pd`      |
| Non-RDMA environment                                                  | `gpt-oss-20b` |
| RDMA-enabled HPC cluster (NVIDIA + InfiniBand/RoCE)                   | `qwen-pd`     |

# Plugin Types Overview

The **`plugins` section** inside the `EndpointPickerConfig` is the *brain* of KServeâ€™s LLM router scheduler.
Each plugin type contributes a different logic for **how incoming inference requests are classified, scored, filtered, and routed** to backend inference pods (endpoints).

In `KServe LLMInferenceService`, the router scheduler defines:

```
apiVersion: inference.networking.x-k8s.io/v1alpha1
kind: EndpointPickerConfig
plugins:
  - type: <plugin-type>
    parameters: ...
schedulingProfiles:
  - name: <profile>
    plugins:
      - pluginRef: <plugin-type>
```

These plugins fall into four functional groups:


| Category | Function                                       |
| -------- | ---------------------------------------------- |
| Handler  | Classify or pre-process the request.           |
| Filter   | Select which endpoints are eligible.           |
| Scorer   | Score eligible endpoints to decide best match. |
| Picker   | Choose the final endpoint among candidates.    |

## **Handler Plugins**

Handlers interpret or classify the request context before any scoring or filtering happens.

## `single-profile-handler`

**Purpose:**
Simplest handler â€” routes all requests through a single scheduling profile (`default`).

**Used in:**`gpt-oss-20b`

**Function:**

* Doesnâ€™t differentiate between request types.
* Suitable for *single-stage* inference (no prefill/decode separation).

**Analogy:** â€œEvery request goes through the same queue.â€

## `pd-profile-handler`

**Purpose:**
Separates requests into *prefill* and *decode* phases for **two-stage LLM serving**.

**Used in:**`qwen-pd`

**Parameters:**

```
parameters:
  threshold: 0
```

**threshold:** Controls how to classify requests.

0 â†’ Always separate (every request has prefill + decode).

Higher values â†’ separate only when request size exceeds threshold.

**Effect:**
Routes requests to:

prefill profile â†’ for initial prompt embedding

decode profile â†’ for token-by-token generation

## `prefill-header-handler`

**Purpose:**
Reads HTTP/gRPC headers to detect prefill requests explicitly (when client sets a header flag).

**Used in:**`qwen-pd`

**Example:**
If client sends a request with header `X-Prefill: true`, this handler assigns it to the `prefill` profile.

**Effect:**
Useful for explicit phase control by the client instead of auto-detection.

## Filter Plugin

Filters decide *which endpoints* are even eligible to serve a given request.

### prefill-filter`

**Purpose:**
Selects endpoints belonging to the **prefill pool** only.

**Used in:**`qwen-pd`

**Function:**
When the active profile is `prefill`, this filter ensures only pods in the prefill deployment are candidates.

###decode-filter`

**Purpose:**
Selects endpoints belonging to the **decode pool** only.

**Used in:**`qwen-pd`

**Function:**
Ensures decode-phase requests go only to decode pool pods (which are configured differently, e.g. lower latency, smaller batch sizes).

## Scorer Plugins

Scorers assign numerical scores to candidate endpoints to decide *which one* is best.
Higher scores mean better routing suitability.

### ğŸ”¹ `queue-scorer`

**Purpose:**
Scores endpoints based on current queue depth (load).

**Used in:**`gpt-oss-20b`

**Logic:**

* Endpoints with fewer queued requests score higher (preferred).
* Prevents overloading a single pod.

### ğŸ”¹ `kv-cache-scorer`

**Purpose:**
Prefers endpoints with **matching KV cache entries** (cached context from previous tokens).

**Used in:**`gpt-oss-20b`

**Function:**

* Looks for partial token history matches in the endpointâ€™s in-memory KV cache.
* Reduces latency by avoiding recomputation of previously cached attention blocks.

**Result:**
Improves performance for streaming or continuation requests.

### ğŸ”¹ `prefix-cache-scorer`

**Purpose:**
More advanced cache scorer â€” matches longer *prompt prefixes* between new and previous requests.

**Used in:**`gpt-oss-20b`

**Parameters:**

```
parameters:
  hashBlockSize: 64
  maxPrefixBlocksToMatch: 256
  lruCapacityPerServer: 39744

```

**Meaning:**

* `hashBlockSize`: Size of token blocks used for hashing prefix segments.
* `maxPrefixBlocksToMatch`: Max number of blocks to compare (affects token limit).
* `lruCapacityPerServer`: Size of prefix cache LRU per endpoint.

**Effect:**
Allows router to route requests to pods that already computed a similar prompt prefix, leveraging prefix reuse to save compute time.

### ğŸ”¹ `max-score-picker`

**Purpose:**
Final decision maker â€” picks the endpoint with the highest total score.

**Used in:**`gpt-oss-20b`

**Function:**
Takes scores from all scorers and picks the endpoint with the best composite score (after weights).

## Picker Plugins

Pickers are the last step â€” they actually **choose** which endpoint to send the request to, based on scoring or randomness.

### ğŸ”¹ `random-picker`

**Purpose:**
Randomly picks an endpoint among filtered candidates.

**Used in:**`qwen-pd`

**Parameters:**

```
parameters:
  maxNumOfEndpoints: 1

```

* `maxNumOfEndpoints`: Limit how many endpoints to pick from.

**Use Case:**
Used in simpler or low-latency routing profiles where deterministic scoring is unnecessary (e.g., random load balancing among decode pods).

### Scheduling Profiles

Each `schedulingProfile` defines **which plugins** apply to each class of requests:

**Example â€” gpt-oss-20b**

```
schedulingProfiles:
- name: default
  plugins:
    - pluginRef: prefix-cache-scorer
      weight: 2.0
    - pluginRef: queue-scorer
      weight: 1.0
    - pluginRef: kv-cache-scorer
      weight: 1.0
    - pluginRef: max-score-picker

```

**Meaning:**
All requests go through:

Cache-based scorers (prefix, kv)

Queue load scorer

Pick the endpoint with max weighted score.

**Example â€” qwen-pd**

```
schedulingProfiles:
- name: prefill
  plugins:
    - pluginRef: prefill-filter
    - pluginRef: random-picker
- name: decode
  plugins:
    - pluginRef: decode-filter
    - pluginRef: random-picker

```

**Meaning:**
Requests first classified as prefill or decode by the handlers, then routed randomly among eligible endpoints in that pool.

`gpt-oss-20b` â€” *Single Pool, Cache-Aware Routing*

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Incoming Inference Req.  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Handler: single-profile-handler â”‚
              â”‚  â†’ All requests use "default"    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚           Scoring Phase (default)          â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ 1ï¸âƒ£ prefix-cache-scorer                    â”‚
         â”‚     â†’ Checks for prompt prefix matches     â”‚
         â”‚ 2ï¸âƒ£ kv-cache-scorer                        â”‚
         â”‚     â†’ Checks for cached KV state reuse     â”‚
         â”‚ 3ï¸âƒ£ queue-scorer                           â”‚
         â”‚     â†’ Prefers endpoints with shorter queue â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Picker: max-score-picker                   â”‚
       â”‚ â†’ Combines weighted scores, selects best   â”‚
       â”‚   endpoint for the request                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Route to best endpoint (Pod)   â”‚
           â”‚ e.g., gpu-node-1 (inference)   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


```

**Summary:**


Single pool routing â†’ Smart cache scoring â†’ Highest score wins.
Good for smaller clusters or when cache locality matters more than multi-stage optimization.

`qwen-pd` â€” *Prefill/Decode Split with RDMA Cache Transfer*

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Incoming Inference Req.  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Handler Phase                               â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚ pd-profile-handler                          â”‚
          â”‚   â†’ Classifies as "prefill" or "decode"     â”‚
          â”‚ prefill-header-handler                      â”‚
          â”‚   â†’ (Optional) reads header for phase info  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ If Prefill Profile                         â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  prefill-filter â†’ keeps only prefill pods   â”‚
          â”‚  random-picker â†’ randomly pick one          â”‚
          â”‚  (Pod in Prefill Pool)                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Prefill Pod        â”‚
                     â”‚ - Builds prompt    â”‚
                     â”‚ - Generates KV cacheâ”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                      (KV cache via RDMA)
                               â”‚
                               â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Decode Profile                              â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  decode-filter â†’ keeps only decode pods      â”‚
          â”‚  random-picker â†’ randomly pick one           â”‚
          â”‚  (Pod in Decode Pool)                        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Decode Pod         â”‚
                     â”‚ - Streams tokens   â”‚
                     â”‚ - Uses KV cache    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Final Response to User  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Summary:**
Two-stage serving:

Prefill stage builds the prompt & KV cache.

Decode stage uses RDMA (RoCE) to fetch that cache and generate tokens efficiently.

Router plugins ensure each request phase goes to the right pool.


| Phase      | `gpt-oss-20b`                       | `qwen-pd`                            |
| ---------- | ----------------------------------- | ------------------------------------ |
| Handler    | Single profile                      | Prefill/Decode split                 |
| Scorer     | Prefix/KV/Queue scoring             | None (simple random pick)            |
| Picker     | max-score-picker                    | random-picker****                    |
| Networking | Standard TCP/IP                     | RDMA (low-latency KV cache transfer) |
| Purpose    | Optimize cache reuse in single pool | Enable 2-stage distributed inference |
